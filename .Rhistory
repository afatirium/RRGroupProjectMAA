TN=tt[1,1]
TP=tt[2,2]
FP=tt[2,1]
FN=tt[1,2]
}
sens[i]=TP/(TP+FN)
spec[i]=TN/(TN+FP)
}
summary(accuracy)
summary(modelloans)
summary(accuracy)
which(max(accuracy)==accuracy)/1000
prob=predict(modelloantrain,test[,c("gender","car","child","annt","cred","single","work","educ","marr")],type="response")
summary(prob)
accuracy=rep(0,1000) ##Niye NA verdi?
sens=rep(0,1000)
spec=rep(0,1000)
for (i in 1:1000)
{
class=ifelse(prob>i/1000,1,0)
accuracy[i]=mean(class==test$def)
tt=table(class,test$def)
if(dim(tt)[1]==1){
TN=tt[1,1]
TP=0
FP=0
FN=tt[1,2]}
else {
TN=tt[1,1]
TP=tt[2,2]
FP=tt[2,1]
FN=tt[1,2]
}
sens[i]=TP/(TP+FN)
spec[i]=TN/(TN+FP)
}
summary(accuracy)
##Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's
##     NA      NA      NA     NaN      NA      NA    1000
which(max(accuracy)==accuracy)/1000
tic()
modelloantrain=glm(def~gender+car+child+annt+cred+single+work+educ+marr
,data=train,family=binomial(link="logit"))
toc()
summary(modelloantrain)
prob=predict(modelloantrain,test[,c("gender","car","child","annt","cred","single","work","educ","marr")],type="response")
summary(prob)
accuracy=rep(0,1000) ##Niye NA verdi?
sens=rep(0,1000)
spec=rep(0,1000)
for (i in 1:1000)
{
class=ifelse(prob>i/1000,1,0)
accuracy[i]=mean(class==test$def)
tt=table(class,test$def)
if(dim(tt)[1]==1){
TN=tt[1,1]
TP=0
FP=0
FN=tt[1,2]}
else {
TN=tt[1,1]
TP=tt[2,2]
FP=tt[2,1]
FN=tt[1,2]
}
sens[i]=TP/(TP+FN)
spec[i]=TN/(TN+FP)
}
summary(accuracy)
##Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's
##     NA      NA      NA     NaN      NA      NA    1000
which(max(accuracy)==accuracy)/1000
cost=rep(0,1000)
sens=rep(0,1000)
spec=rep(0,1000)
for (i in 1:1000){
class=ifelse(prob>i/1000,1,0)
tt=table(class,test$def)
if(dim(tt)[1]==1){
TN=tt[1,1]
TP=0
FP=0
FN=tt[1,2]}
else {
TN=tt[1,1]
TP=tt[2,2]
FP=tt[2,1]
FN=tt[1,2]
}
sens[i]=TP/(TP+FN)
spec[i]=TN/(TN+FP)
cost[i]=10*FP+200*FN
}
plot(cost,type="l",lwd="2",col="red")
which(cost==min(cost))/1000   # 0.048
cost[which(cost==min(cost))] #631050
class=ifelse(prob>0.041,1,0)
sens[41] #0.9738347
spec[41] #0.05730098
mean(class==test$def) #NA
summary(prob) ##bunu qurdum, amma analiz ede bilmedim ki, bu neticelerden ne qenaete gelmeliyem
summary(modelloantrain)
# Define the train control with 5-fold cross-validation
ctrl <- trainControl(method = "cv", number = 5)
library(beanplot)
library(summarytools)
library(ggplot2)
library(tidyr)
library(gridExtra)
library(scales)
library(zoo)
library(dplyr)
library(gbm)
library(glmnet)
library(xgboost)
library(e1071)
# Define the train control with 5-fold cross-validation
ctrl <- trainControl(method = "cv", number = 5)
### Support Vector Regression (SVR Model)
# Fit the SVR model
svr <- svm(y_train ~ ., data = X_train, kernel = "radial", cost = 100000)
train$LotFrontage <- train$LotFrontage^(1/3)
train$LotArea <- train$LotArea^(1/6)
train$MasVnrArea <- train$MasVnrArea^(1/1.5)
train$BsmtFinSF1 <- train$BsmtFinSF1^(1/1.4)
train$BsmtUnfSF <- train$BsmtUnfSF^(1/1.5)
train$TotalBsmtSF <- train$TotalBsmtSF^(1/1.4)
train$X1stFlrSF <- train$X1stFlrSF^(1/4)
train$GrLivArea <- train$GrLivArea^(1/4.5)
train$GarageArea <- train$GarageArea^(1/1.1)
train$WoodDeckSF <- train$WoodDeckSF^(1/1.2)
train$OpenPorchSF <- train$OpenPorchSF^(1/2.5)
setwd('C:\\Users\\Afat\\Documents\\GitHub\\RRGroupProjectMAA') #Afet add setwd for herself
train <- read.csv("train.csv", h=T)
test <- read.csv("test.csv", h=T)
row.names(train) <- train$Id
train$Id <- NULL
row.names(test) <- test$Id
test$Id <- NULL
missing_values <- colSums(is.na(train))
missing_values <- missing_values[order(-missing_values)]
# Display the top 20 variables with the highest number of missing values
head(missing_values, 20)
# Check the missing values for test dataset
missing_values <- colSums(is.na(test))
missing_values <- missing_values[order(-missing_values)]
# Display the top 35 variables with the highest number of missing values
head(missing_values, 35)
##Note: For both dataset some variables have a lot missing values: PoolQC, MiscFeature, Alley, Fence, FireplaceQu, LotFrontage
# Drop the "PoolQC" column from the train dataset
train <- train[, !(colnames(train) %in% "PoolQC")]
# Drop the "PoolQC" column from the test dataset
test <- test[, !(colnames(test) %in% "PoolQC")]
#########Fill the missing variables for the categorical variables:
# Define the features with missing values
obj_NA <- c("Alley", "BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1",
"BsmtFinType2", "FireplaceQu", "GarageType", "GarageFinish",
"GarageQual", "GarageCond", "Fence", "MiscFeature")
# Fill in missing values with "NA" for the specified features in the train dataset
for (i in obj_NA) {
train[, i][is.na(train[, i])] <- "NA"
}
# Fill in missing values with "NA" for the specified features in the test dataset
for (i in obj_NA) {
test[, i][is.na(test[, i])] <- "NA"
}
# Fill in missing values for the MSZoning, MasVnrType, Electrical, KitchenQual, Functional and SaleType
obj_mode <- c("MSZoning", "MasVnrType", "Electrical", "KitchenQual", "Functional", "SaleType")
train$MSZoning[train$MSZoning == "C (all)"] <- "C"
test$MSZoning[test$MSZoning == "C (all)"] <- "C"
for (i in obj_mode) {
train[[i]][is.na(train[[i]])] <- names(which.max(table(train[[i]])))
test[[i]][is.na(test[[i]])] <- names(which.max(table(test[[i]])))
}
# Fill in missing values for the Utilities feature
test$Utilities[is.na(test$Utilities)] <- "NoSeWa"
# Fill in missing values for the Exterior1st feature
test$Exterior1st[is.na(test$Exterior1st)] <- "ImStucc"
# Fill in missing values for the Exterior2nd feature
test$Exterior2nd[is.na(test$Exterior2nd)] <- "Other"
##Clean continues variables:
##Fill missing variables
# # Fill in missing values for the LotFrontage and MasVnrArea features
obj_median <- c("LotFrontage", "MasVnrArea")
for (i in obj_median) {
train[[i]] <- na.aggregate(train[[i]], FUN = median)
test[[i]] <- na.aggregate(test[[i]], FUN = median)
}
#  Fill in missing values for the BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath, BsmtHalfBath, GarageCars and GarageArea features
Mode <- function(x) {
ux <- unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
obj_mode <- c("BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "BsmtFullBath", "BsmtHalfBath", "GarageCars", "GarageArea")
for (i in obj_mode) {
test[[i]] <- replace(test[[i]], is.na(test[[i]]), Mode(test[[i]]))
}
# Fill in missing values for the GarageYrBlt feature
train$GarageYrBlt <- ifelse(is.na(train$GarageYrBlt), train$YearBuilt, train$GarageYrBlt)
test$GarageYrBlt <- ifelse(is.na(test$GarageYrBlt), test$YearBuilt, test$GarageYrBlt)
# Converting dtype to 'int64'
int_list <- c("LotFrontage", "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "BsmtFullBath", "BsmtHalfBath", "GarageYrBlt", "GarageCars", "GarageArea")
for (i in int_list) {
train[[i]] <- as.integer(train[[i]])
test[[i]] <- as.integer(test[[i]])
}
#check the shape of dataset
dim(train)
train$LotFrontage <- train$LotFrontage^(1/3)
train$LotArea <- train$LotArea^(1/6)
train$MasVnrArea <- train$MasVnrArea^(1/1.5)
train$BsmtFinSF1 <- train$BsmtFinSF1^(1/1.4)
train$BsmtUnfSF <- train$BsmtUnfSF^(1/1.5)
train$TotalBsmtSF <- train$TotalBsmtSF^(1/1.4)
train$X1stFlrSF <- train$X1stFlrSF^(1/4)
train$GrLivArea <- train$GrLivArea^(1/4.5)
train$GarageArea <- train$GarageArea^(1/1.1)
train$WoodDeckSF <- train$WoodDeckSF^(1/1.2)
train$OpenPorchSF <- train$OpenPorchSF^(1/2.5)
features <- colnames(train)
for (feature in features) {
if (class(train[[feature]]) == "character") {
train[[feature]] <- as.integer(factor(train[[feature]]))
test[[feature]] <- as.integer(factor(test[[feature]]))
}
}
# Drop unnecessary features
X <- train[, !colnames(train) %in% c("SalePrice")]
# Create the target variable
y <- train$SalePrice
# Set the seed for reproducibility
set.seed(42)
# Split the data into training and test sets
train_indices <- sample(1:nrow(train), size = round(0.8 * nrow(train)), replace = FALSE)
X_train <- X[train_indices, ]
y_train <- y[train_indices]
X_test <- X[-train_indices, ]
y_test <- y[-train_indices]
### Standardize Features
# Fit the scaler on the training data
scaler <- scale(X_train)
# Apply the scaler to the training data
X_train_scaled <- scaler
# Apply the scaler to the test data
X_test_scaled <- scale(X_test, center = attr(scaler, "scaled:center"), scale = attr(scaler, "scaled:scale"))
##
# Calculate the mean and standard deviation from the training set
scale_params <- apply(X_train, 2, mean)
scale_sd <- apply(X_train, 2, sd)
# Scale the training set
X_train_scaled <- scale(X_train, center = scale_params, scale = scale_sd)
# Scale the test set using the scaling parameters from the training set
X_test_scaled <- scale(X_test, center = scale_params, scale = scale_sd)
# build finction for loss function
rmse_cv <- function(model) {
rmse <- sqrt(-mean(cross_val_score(model, X, y, scoring = "neg_mean_squared_error", cv = 5)))
return(round(rmse, 4))
}
evaluation <- function(y, predictions) {
mae <- mean(abs(y - predictions))
mse <- mean((y - predictions)^2)
rmse <- sqrt(mse)
r_squared <- 1 - (sum((y - predictions)^2) / sum((y - mean(y))^2))
return(list(mae = round(mae, 4), mse = round(mse, 4), rmse = round(rmse, 4), r_squared = round(r_squared, 4)))
}
models <- data.frame(Model = character(),
MAE = numeric(),
MSE = numeric(),
RMSE = numeric(),
R2_Score = numeric(),
RMSE_CV = numeric(),
stringsAsFactors = FALSE)
# Gradient Boosting Model
# Fit the Gradient Boosting model
g_boost <- gbm(y_train ~ ., data = X_train, n.trees = 100, interaction.depth = 3)
# Predict on the test set
predictions <- predict(g_boost, newdata = X_test, n.trees = 100)
# Calculate evaluation metrics
metrics <- evaluation(y_test, predictions)
mae <- metrics$mae
mse <- metrics$mse
rmse <- metrics$rmse
r_squared <- metrics$r_squared
cat("MAE:", mae, "\n")
cat("MSE:", mse, "\n")
cat("RMSE:", rmse, "\n")
cat("R2 Score:", r_squared, "\n")
cat("----------------------------------\n")
# Perform cross-validation using cv.glmnet
cv_fit <- cv.glmnet(x = as.matrix(X), y = y, nfolds = 5, alpha = 0.5)
# Calculate the RMSE
rmse_cv <- sqrt(cv_fit$cvm)
# Print the RMSE for each fold
cat("RMSE (Cross-Validation):", rmse_cv, "\n")
# Get the average RMSE
mean_rmse_cv <- mean(rmse_cv)
cat("Mean RMSE (Cross-Validation):", mean_rmse_cv, "\n")
# Append the average RMSE to the models data frame
models$`RMSE (Cross-Validation)`[models$Model == "GradientBoosting"] <- mean_rmse_cv
# Create a new row for the model results
new_row <- data.frame(Model = "GradientBoosting",
MAE = mae,
MSE = mse,
RMSE = rmse,
R2_Score = r_squared,
RMSE_CV = mean_rmse_cv)
# Append the new row to the models data frame
models <- rbind(models, new_row)
# Convert the data to DMatrix format
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
dtest <- xgb.DMatrix(data = as.matrix(X_test))
# Set the parameters for the XGBoost model
params <- list(
objective = "reg:squarederror",
eval_metric = "rmse"
)
# Train the XGBoost model
Xg_boost <- xgb.train(params = params, data = dtrain, nrounds = 100)
# Predict on the test set
predictions <- predict(Xg_boost, newdata = dtest)
# Calculate evaluation metrics
mae <- mean(abs(predictions - y_test))
mse <- mean((predictions - y_test)^2)
rmse <- sqrt(mse)
r_squared <- 1 - mse / var(y_test)
cat("MAE:", mae, "\n")
cat("MSE:", mse, "\n")
cat("RMSE:", rmse, "\n")
cat("R2 Score:", r_squared, "\n")
cat("----------------------------------\n")
# Perform cross-validation to calculate RMSE
cv_result <- xgb.cv(params = params, data = dtrain, nfold = 5, nrounds = 100)
rmse_cross_val <- min(cv_result$evaluation_log$test_rmse_mean)
cat("RMSE Cross-Validation:", rmse_cross_val, "\n")
# Create a new row for the model results
new_row <- data.frame(Model = "XGradientBoosting",
MAE = mae,
MSE = mse,
RMSE = rmse,
R2_Score = r_squared,
RMSE_CV = rmse_cross_val)
# Append the new row to the models data frame
models <- rbind(models, new_row)
models
# Fit the SVR model
svr <- svm(y_train ~ ., data = X_train, kernel = "radial", cost = 100000)
# Predict on the test set
predictions <- predict(svr, newdata = X_test)
# Calculate evaluation metrics
metrics <- evaluation(y_test, predictions)
mae <- metrics$mae
mse <- metrics$mse
rmse <- metrics$rmse
r_squared <- metrics$r_squared
cat("MAE:", mae, "\n")
cat("MSE:", mse, "\n")
cat("RMSE:", rmse, "\n")
cat("R2 Score:", r_squared, "\n")
cat("----------------------------------\n")
# Perform cross-validation to calculate RMSE
library(caret)
# Define the train control with 5-fold cross-validation
ctrl <- trainControl(method = "cv", number = 5)
# Perform cross-validation using train() function
cv_fit <- train(x = as.matrix(X), y = y, method = "svmRadial", trControl = ctrl, preProcess = c("center", "scale"), tuneGrid = data.frame(C = 100000))
# Calculate the RMSE
rmse_cv <- cv_fit$results$RMSE
# Print the RMSE for each fold
cat("RMSE (Cross-Validation):", rmse_cv, "\n")
# Get the average RMSE
mean_rmse_cv <- mean(rmse_cv)
cat("Mean RMSE (Cross-Validation):", mean_rmse_cv, "\n")
# Append the average RMSE to the models data frame
models$`RMSE (Cross-Validation)`[models$Model == "SVR"] <- mean_rmse_cv
# Create a new row for the model results
new_row <- data.frame(Model = "SVR",
MAE = mae,
MSE = mse,
RMSE = rmse,
R2_Score = r_squared,
RMSE_CV = mean_rmse_cv)
# Append the new row to the models data frame
models <- rbind(models, new_row)
# Fit the SVR model
svr <- svm(y_train ~ ., data = X_train, kernel = "radial", cost = 100000)
# Predict on the test set
predictions <- predict(svr, newdata = X_test)
# Calculate evaluation metrics
metrics <- evaluation(y_test, predictions)
mae <- metrics$mae
mse <- metrics$mse
rmse <- metrics$rmse
r_squared <- metrics$r_squared
cat("MAE:", mae, "\n")
cat("MSE:", mse, "\n")
cat("RMSE:", rmse, "\n")
cat("R2 Score:", r_squared, "\n")
cat("----------------------------------\n")
# Perform cross-validation to calculate RMSE
library(caret)
# Define the train control with 5-fold cross-validation
ctrl <- trainControl(method = "cv", number = 5)
# Perform cross-validation using train() function
cv_fit <- train(x = as.matrix(X), y = y, method = "svmRadial", trControl = ctrl, preProcess = c("center", "scale"), tuneGrid = data.frame(C = 100000))
# Calculate the RMSE
rmse_cv <- cv_fit$results$RMSE
# Print the RMSE for each fold
cat("RMSE (Cross-Validation):", rmse_cv, "\n")
# Get the average RMSE
mean_rmse_cv <- mean(rmse_cv)
cat("Mean RMSE (Cross-Validation):", mean_rmse_cv, "\n")
# Append the average RMSE to the models data frame
models$`RMSE (Cross-Validation)`[models$Model == "SVR"] <- mean_rmse_cv
# Create a new row for the model results
new_row <- data.frame(Model = "SVR",
MAE = mae,
MSE = mse,
RMSE = rmse,
R2_Score = r_squared,
RMSE_CV = mean_rmse_cv)
# Append the new row to the models data frame
models <- rbind(models, new_row)
models
### Support Vector Regression (SVR Model)
# Training the SVR model
svr <- SVR(C = 100000)
svr.fit(X_train, y_train)
### Support Vector Regression (SVR Model)
install.packages("e1071")
install.packages("e1071")
library(e1071)
# Training the SVR model
svr <- svm(y_train ~ ., data = X_train, kernel = "radial", cost = 100000)
# Making predictions
predictions <- predict(svr, newdata = X_test)
# Evaluating the model
mae <- mean(abs(predictions - y_test))
mse <- mean((predictions - y_test)^2)
rmse <- sqrt(mse)
r_squared <- 1 - sum((y_test - predictions)^2) / sum((y_test - mean(y_test))^2)
print("MAE:", mae)
print("MSE:", mse)
print("RMSE:", rmse)
# Training the SVR model
svr <- svm(y_train ~ ., data = X_train, kernel = "radial", cost = 100000)
# Making predictions
predictions <- predict(svr, newdata = X_test)
# Evaluating the model
mae <- mean(abs(predictions - y_test))
mse <- mean((predictions - y_test)^2)
rmse <- sqrt(mse)
r_squared <- 1 - sum((y_test - predictions)^2) / sum((y_test - mean(y_test))^2)
cat("MAE:", as.character(mae), "\n")
cat("MSE:", as.character(mse), "\n")
cat("RMSE:", as.character(rmse), "\n")
cat("R2 Score:", as.character(r_squared), "\n")
cat("-" * 30, "\n")
# Calculating RMSE through cross-validation
rmse_cross_val <- sqrt(crossprod(y_train - predict(svr, newdata = X_train)) / length(y_train))
cat(paste(rep("-", 30), collapse = ""), "\n")
# Calculating RMSE through cross-validation
rmse_cross_val <- sqrt(crossprod(y_train - predict(svr, newdata = X_train)) / length(y_train))
cat("RMSE Cross-Validation:", as.character(rmse_cross_val), "\n")
# Creating a new row with the evaluation metrics
new_row <- data.frame(
Model = "SVR",
MAE = mae,
MSE = mse,
RMSE = rmse,
R2_Score = r_squared,
RMSE_Cross_Validation = rmse_cross_val
)
# Appending the new row to the models dataframe
models <- rbind(models, new_row)
models
# Creating a new row with the evaluation metrics
new_row <- data.frame(
Model = "SVR",
MAE = mae,
MSE = mse,
RMSE = rmse,
R2_Score = r_squared,
RMSE_CV = rmse_cross_val
)
# Appending the new row to the models dataframe
models <- rbind(models, new_row)
models
models
# Fit the SVR model
svr <- svm(X_train, y_train, kernel = "radial", cost = 100000)
# Predict on the test set
predictions <- predict(svr, X_test)
# Calculate evaluation metrics
mae <- mean(abs(predictions - y_test))
mse <- mean((predictions - y_test)^2)
rmse <- sqrt(mse)
r_squared <- 1 - mse / var(y_test)
cat("MAE:", mae, "\n")
cat("MSE:", mse, "\n")
cat("RMSE:", rmse, "\n")
cat("R2 Score:", r_squared, "\n")
cat("----------------------------------\n")
# Perform cross-validation to calculate RMSE
install.packages("caret", dependencies = TRUE)
update.packages()
library(caret)
# Define the train control with 5-fold cross-validation
ctrl <- trainControl(method = "cv", number = 5)
library(caret)
# Define the train control with 5-fold cross-validation
ctrl <- trainControl(method = "cv", number = 5)
