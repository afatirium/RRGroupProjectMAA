---
title: "Project_MAA"
subtitle: "House pricing Model"
author: "Allahverdili Minaya, Aliev Aziz, Ibadova Afet"
date: "06/15/2023"
format: html
execute:
  echo: fenced
editor: visual
---

# Preparing Libraries

```{r}

#install libraries
#install.packages("beanplot")
#install.packages("summarytools")
#install.packages("ggplot2")
#install.packages("tidyr")
#install.packages("gridExtra")
#install.packages("scales")
#install.packages("zoo")
#install.packages("dplyr")
#install.packages("gbm")
#install.packages("glmnet")
#install.packages("xgboost")
#install.packages("e1071")
#install.packages("caret")
#install.packages("rpart")

##Import libraries
library(beanplot)
library(summarytools)
library(ggplot2)
library(tidyr)
library(gridExtra)
library(scales)
library(zoo)
library(dplyr)
library(gbm)
library(glmnet)
library(xgboost)
library(e1071)
library(caret)
library(rpart)
library(randomForest)
library(caret)
library(adabag)
```

# Load Dataset

```{r}

getwd()
train <- read.csv("train.csv", h=T)
test <- read.csv("test.csv", h=T)
row.names(train) <- train$Id
train$Id <- NULL
row.names(test) <- test$Id
test$Id <- NULL
```

# Data Exploration and Preprocessing

## 1. Investigate TARGET value distribution

```{r}
par(mfrow = c(2, 2))
par(mar = c(4, 4, 2, 1)) # Adjusting the margins for better visualization

# Subplot 1
hist(train$SalePrice, breaks = 50, main = "Sales Price Distribution", xlab = "Sale Price",
     ylab = "Counts", col = "lightblue", border = "white")
abline(v = mean(train$SalePrice), lty = 2, col = "red", lwd = 2)
abline(v = median(train$SalePrice), lty = 3, col = "green", lwd = 2)
legend("topright", legend = c("Mean", "Median"), lty = c(2, 3), col = c("red", "green"))

# Subplot 2
plot(train$SalePrice, rnorm(nrow(train), mean = 7, sd = 0.2), main = "Sales Price Distribution",
     xlab = "Sale Price", ylab = "Random Values", pch = 16, col = "blue", cex = 0.6)

# Subplot 3
boxplot(train$SalePrice, main = "Sales Price Distribution", ylab = "Sale Price",
        col = "lightgreen")


```

**Note:** Target is is continuous, and the distribution is skewed to the right. Mostly house price ranges from 100,000 to 200,000.

## 2. Investigate Data Types

```{r}
str(train)
```

-   Some variables, such as **YearBuilt, YearRemodAdd, GarageYrBlt, YrSold, MoSold** are numeric variables, which is related to the year.

-   Some variables have a lot of null value, it can be cleaned.

### 2.1. Explore the categorical variables

```{r}
cat_vars <- names(train)[sapply(train, is.character)]
cat_vars 
```

**MSSubClass** is also categorical by definition, despite its numeric values. MSSubClass should be added to the list of categorical variables.

```{r}
cat_vars <- c(cat_vars, "MSSubClass")
length(cat_vars)
```

```{r}
train[cat_vars] <- lapply(train[cat_vars], as.character)
t(sapply(train[cat_vars], summary))
```

**Summary for categorical variables:**

```{r}
dfSummary(train[cat_vars]) 
```

### 2.2. Explore the continues variables

```{r}
stat_cont <- summary(train[, sapply(train, is.numeric)])
stat_cont
```

```{r}
tr_cont <- train[, !sapply(train, is.character)]
head(tr_cont)
```

**Plotting heatmap for visualizing correlation:**

```{r}
corr <- cor(tr_cont)
melted_corr <- as.data.frame(as.table(corr))
colnames(melted_corr) <- c("Var1", "Var2", "value")



# Increase the plot size
options(repr.plot.width = 30, repr.plot.height = 30)

ggplot(melted_corr, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +  # Set the tile border color to white
  scale_fill_gradient(low = "red", high = "yellow") +
  labs(title = "Correlation Heatmap") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),  # Rotate x-axis labels
        panel.grid = element_blank())  # Remove grid lines
```

**Histograms for continues variables:**

```{r}
# Select continuous features to plot
features <- names(tr_cont)[!(names(tr_cont) %in% c("Id", "OverallQual", "OverallCond", "YearBuilt", "YearRemodAdd",
                                                   "BsmtFullBath", "BsmtHalfBath", "FullBath", "HalfBath",
                                                   "BedroomAbvGr", "KitchenAbvGr", "TotRmsAbvGrd", "Fireplaces",
                                                   "GarageYrBlt", "GarageCars", "MoSold", "YrSold", "SalePrice"))]

for (i in 1:length(features)) {
  hist(tr_cont[[features[i]]], main = paste("Histogram for", features[i]), xlab = features[i], col ="lightblue")
}
```

### 2.3. Missing values

Check the missing values for train dataset:

```{r}

missing_values <- colSums(is.na(train))

missing_values <- missing_values[order(-missing_values)]

```

Display the top 20 variables with the highest number of missing values

```{r}
head(missing_values, 20)
```

Check the missing values for test dataset:

```{r}

missing_values <- colSums(is.na(test))

missing_values <- missing_values[order(-missing_values)]
```

Display the top 35 variables with the highest number of missing values

```{r}
head(missing_values, 35)
```

***Note:*** For both dataset some variables have a lot missing values: PoolQC, MiscFeature, Alley, Fence, FireplaceQu, LotFrontage

Drop the "PoolQC" column from the train dataset

```{r}
train <- train[, !(colnames(train) %in% "PoolQC")]
```

Drop the "PoolQC" column from the test dataset:

```{r}
test <- test[, !(colnames(test) %in% "PoolQC")]
```

**Fill the missing variables for the categorical variables:**

Define the features with missing values:

```{r}
obj_NA <- c("Alley", "BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1",

"BsmtFinType2", "FireplaceQu", "GarageType", "GarageFinish",

"GarageQual", "GarageCond", "Fence", "MiscFeature")
```

Fill in missing values with "NA" for the specified features in the train dataset

```{r}
for (i in obj_NA) {

train[, i][is.na(train[, i])] <- "NA"

}
```

Fill in missing values with "NA" for the specified features in the test dataset:

```{r}
for (i in obj_NA) {

test[, i][is.na(test[, i])] <- "NA"

}
```

Fill in missing values for the MSZoning, MasVnrType, Electrical, KitchenQual, Functional and SaleType:

```{r}
obj_mode <- c("MSZoning", "MasVnrType", "Electrical", "KitchenQual", "Functional", "SaleType")

train$MSZoning[train$MSZoning == "C (all)"] <- "C"
test$MSZoning[test$MSZoning == "C (all)"] <- "C"
```

```{r}
for (i in obj_mode) {
  train[[i]][is.na(train[[i]])] <- names(which.max(table(train[[i]])))
  test[[i]][is.na(test[[i]])] <- names(which.max(table(test[[i]])))
}
```

Fill in missing values for the Utilities feature

```{r}
test$Utilities[is.na(test$Utilities)] <- "NoSeWa"
```

Fill in missing values for the Exterior1st feature

```{r}
test$Exterior1st[is.na(test$Exterior1st)] <- "ImStucc"
```

Fill in missing values for the Exterior2nd feature

```{r}
test$Exterior2nd[is.na(test$Exterior2nd)] <- "Other"
```

**Clean continues variables:**

Fill missing variables

Fill in missing values for the LotFrontage and MasVnrArea features

```{r}
obj_median <- c("LotFrontage", "MasVnrArea")

for (i in obj_median) {
  train[[i]] <- na.aggregate(train[[i]], FUN = median)
  test[[i]] <- na.aggregate(test[[i]], FUN = median)
}
```

Fill in missing values for the BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath, BsmtHalfBath, GarageCars and GarageArea features

```{r}
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

obj_mode <- c("BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "BsmtFullBath", "BsmtHalfBath", "GarageCars", "GarageArea")

for (i in obj_mode) {
  test[[i]] <- replace(test[[i]], is.na(test[[i]]), Mode(test[[i]]))
}
```

Fill in missing values for the GarageYrBlt feature

```{r}
train$GarageYrBlt <- ifelse(is.na(train$GarageYrBlt), train$YearBuilt, train$GarageYrBlt)
test$GarageYrBlt <- ifelse(is.na(test$GarageYrBlt), test$YearBuilt, test$GarageYrBlt)
```

Converting dtype to 'int64':

```{r}
int_list <- c("LotFrontage", "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "BsmtFullBath", "BsmtHalfBath", "GarageYrBlt", "GarageCars", "GarageArea")

for (i in int_list) {
  train[[i]] <- as.integer(train[[i]])
  test[[i]] <- as.integer(test[[i]])
}
```

check the shape of dataset:

```{r}
dim(train)
```

### 2.4. Feature Selection

#### Correlation Analysis

Filter numeric variables

```{r}
numeric_vars <- sapply(train, is.numeric)

# Calculate correlation matrix
correlations <- cor(train[, numeric_vars])

# Sort correlations with 'SalePrice'
sorted_correlations <- sort(correlations[,"SalePrice"], decreasing = TRUE)
```

Display the most positive correlations:

```{r}
cat("Most Positive Correlations:\n")
print(tail(sorted_correlations, 10))
```

Display the most negative correlations:

```{r}
cat("\nMost Negative Correlations:\n")
print(head(sorted_correlations, 10))
```

**Area vs Sale Price**

```{r}

par(mfrow=c(3, 6), mar=c(5, 4, 2, 1)) # Set the layout of subplots

labels <- c("LotArea","MasVnrArea","BsmtFinSF1","BsmtFinSF2",
            "BsmtUnfSF","TotalBsmtSF","X1stFlrSF","X2ndFlrSF",
            "LowQualFinSF","GrLivArea","GarageArea","WoodDeckSF",
            "OpenPorchSF","EnclosedPorch","X3SsnPorch","ScreenPorch","PoolArea")
desc <- c("Lot Size","Masonry Seneer Area","Basement Type 1 Finished",
          "Basement Type 2 Finished","Unfinished Basement Area",
          "Total Basement Area","First Floor","Second Floor",
          "Low Quality Finished","Above Grade Living Area",
          "Size of Garage","Wood Deck Area","Open Porch Area",
          "Enclosed Porch Area","Three Season Porch Area","Screen Porch Area","Pool Area")

for (i in 1:length(labels)) {
  if (i <= 17) {
    plot(train[[labels[i]]], train[["SalePrice"]],
         xlab = desc[i], ylab = "Sale Price",
         main = desc[i], pch = 16)
  }
}
```

It is clearly seen that there is positive correlation between price and areas i.e. if the area increases price will also increases expect 3SsnPorch (three season porch) and PoolArea.

**Neighborhood vs Sale Price**

```{r}
ggplot(train, aes(x = Neighborhood, y = SalePrice)) +
  geom_bar(stat = "identity") +
  labs(x = NULL, y = "Sale Price") +
  ggtitle("Neighborhood") +
  scale_y_continuous(labels = scales::comma) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Note: We can see that the prices of the house having NoRidge, NridgHt, StoneBr and Timber neighborhoods are very high.

**Building type, House Style**

```{r}
labels <- c("BldgType", "HouseStyle")

plt <- ggplot(train, aes(x = train[[labels[1]]], y = train[["SalePrice"]])) +
  geom_bar(stat = "identity") +
  labs(x = NULL) +
  ggtitle(labels[1])

plt <- plt + facet_wrap(~ train[[labels[2]]], nrow = 1)

plt <- plt + theme_minimal() +
  scale_y_continuous(labels = scales::comma) +
  theme(plot.title = element_text(size = 14),
        axis.text.x = element_text(angle = 90, hjust = 1))

print(plt)
```

**House Quality**

```{r}
par(mfrow = c(2, 4))
labels <- c("OverallQual", "ExterQual", "BsmtQual", "HeatingQC", "KitchenQual", "FireplaceQu", "GarageQual")
col <- 1

for (i in labels) {
  if (col < 9) {
    plot_col <- col %% 4
    if (plot_col == 0) {
      plot_col <- 4
    }
    plot_row <- ceiling(col / 4)
    plot_index <- (plot_row - 1) * 4 + plot_col
    
    barplot(tapply(train$SalePrice, train[, i], mean), xlab = i, ylab = "SalePrice")
    title(main = i)
  }
  
  col <- col + 1
}
```

**Note:** It can be seen that there is positive relation between quality and price. Price increases according to the quality.

**House condition**

```{r}
labels <- c("OverallCond", "ExterCond", "BsmtCond", "GarageCond")
cols <- 1

plots <- list()

for (i in labels) {
  if (cols < 5) {
    plt <- ggplot(train, aes(x = train[[i]], y = train[["SalePrice"]])) +
      geom_bar(stat = "identity") +
      labs(x = NULL) +
      ggtitle(i) +
      theme_minimal() +
      theme(plot.title = element_text(size = 14),
            axis.text.x = element_text(angle = 90, hjust = 1)) +
      scale_y_continuous(labels = scales::comma)
    
    plots[[cols]] <- plt
  }
  cols <- cols + 1
}

grid.arrange(grobs = plots, nrow = 2, ncol = 2, top = "Subplots")
```

**Note:** Prices of the house having Ex (excellent) and Gd (good) condition are very high.

**Bathroom condition**

```{r}
# Custom function to format labels in millions
formatMillions <- function(x) {
  paste0(format(x / 1e6, big.mark = ","), "")
}

# Create individual bar plots
plot1 <- ggplot(train, aes(x = BsmtFullBath, y = SalePrice)) +
  geom_bar(stat = "identity") +
  labs(x = NULL, y = "Sale Price (in million)") +
  ggtitle("BsmtFullBath") +
  scale_y_continuous(labels = formatMillions)

plot2 <- ggplot(train, aes(x = BsmtHalfBath, y = SalePrice)) +
  geom_bar(stat = "identity") +
  labs(x = NULL, y = "Sale Price (in million)") +
  ggtitle("BsmtHalfBath") +
  scale_y_continuous(labels = formatMillions)

plot3 <- ggplot(train, aes(x = FullBath, y = SalePrice)) +
  geom_bar(stat = "identity") +
  labs(x = NULL, y = "Sale Price (in million)") +
  ggtitle("FullBath") +
  scale_y_continuous(labels = formatMillions)

plot4 <- ggplot(train, aes(x = HalfBath, y = SalePrice)) +
  geom_bar(stat = "identity") +
  labs(x = NULL, y = "Sale Price (in million)") +
  ggtitle("HalfBath") +
  scale_y_continuous(labels = formatMillions)

# Arrange the plots in a grid
grid.arrange(plot1, plot2, plot3, plot4, ncol = 2)
```

**Roof style and material**

```{r}
par(mfrow = c(1, 2))
labels <- c("RoofStyle", "RoofMatl")

for (i in 1:length(labels)) {
  if (i <= 2) {
    barplot(tapply(train$SalePrice, train[, labels[i]], mean), xlab = labels[i], ylab = "SalePrice")
    title(main = labels[i])
  }
}
```

**Basement**

```{r}
# Create a list of labels
labels <- c("BsmtExposure", "BsmtFinType1", "BsmtFinType2")

# Create an empty list to store the plots
plots <- list()

# Loop through the labels and create bar plots
for (i in seq_along(labels)) {
  plot <- ggplot(train, aes_string(x = labels[i], y = "SalePrice")) +
    geom_bar(stat = "identity") +
    labs(x = NULL, y = "Sale Price") +
    ggtitle(labels[i]) +
    scale_y_continuous(labels = dollar_format(scale = 1e-6, prefix = "$"))
  
  plots[[i]] <- plot
}


# Arrange the plots in a grid
grid.arrange(grobs = plots, ncol = 3)
```

**Garage**

```{r}
par(mfrow = c(1, 3))
labels <- c("GarageType", "GarageFinish", "GarageCars")

for (i in 1:length(labels)) {
  if (i <= 3) {
    barplot(tapply(train$SalePrice, train[, labels[i]], mean), xlab = labels[i], ylab = "SalePrice")
    title(main = labels[i])
  }
}
```

**Month and Year of Sold**

```{r}
par(mfrow = c(1, 3))
yearsold <- c(2006:2010)
monthsold <- c("January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December")
years <- table(train$YrSold)
months <- table(train$MoSold)

pie(years, labels = yearsold, explode = c(0.1, 0.1, 0.1, 0.1, 0.1), col = rainbow(length(yearsold)), main = "Year Sold", cex.main = 1.2)
pie(months, labels = monthsold, explode = rep(0.1, 12), col = rainbow(length(monthsold)), main = "Month Sold", cex.main = 1.2)
```

**Note:** Maximum houses were sold in year 2009 and in month of June. We can also see a decline in sales from 2009 to 2010.

### 2.5.Transform Skewed Features

Plot histogram for each continuous feature to see if a transformation is necessary

```{r}
par(mfrow = c(4, 5), mar = c(4, 4, 2, 1), oma = c(0, 0, 2, 0))
features <- colnames(train)

col <- 1
for (feature in features) {
  if (class(train[[feature]]) != "character") {
    if (!(feature %in% c("OverallQual", "OverallCond", "YearBuilt", "YearRemodAdd", "BsmtFullBath", "BsmtHalfBath", "FullBath", "HalfBath", "BedroomAbvGr", "KitchenAbvGr", "TotRmsAbvGrd", "Fireplaces", "GarageYrBlt", "GarageCars", "MoSold", "YrSold", "SalePrice"))) {
      if (col < 21) {
        hist(train[[feature]], main = paste("Histogram for", feature), xlab = "", ylab = "", col = "skyblue")
      }
      col <- col + 1
    }
  }
}
```

Box-Cox Power Transformation:

```{r}
train$LotFrontage <- train$LotFrontage^(1/3)
train$LotArea <- train$LotArea^(1/6)
train$MasVnrArea <- train$MasVnrArea^(1/1.5)
train$BsmtFinSF1 <- train$BsmtFinSF1^(1/1.4)
train$BsmtUnfSF <- train$BsmtUnfSF^(1/1.5)
train$TotalBsmtSF <- train$TotalBsmtSF^(1/1.4)
train$X1stFlrSF <- train$X1stFlrSF^(1/4)
train$GrLivArea <- train$GrLivArea^(1/4.5)
train$GarageArea <- train$GarageArea^(1/1.1)
train$WoodDeckSF <- train$WoodDeckSF^(1/1.2)
train$OpenPorchSF <- train$OpenPorchSF^(1/2.5)
```

### 2.6.Convert Categorical Features To Numeric

```{r}
features <- colnames(train)

for (feature in features) {
  if (class(train[[feature]]) == "character") {
    train[[feature]] <- as.integer(factor(train[[feature]]))
    test[[feature]] <- as.integer(factor(test[[feature]]))
  }
}
```

## Split Into Train And Test Set

Drop unnecessary features

```{r}
X <- train[, !colnames(train) %in% c("SalePrice")]

# Create the target variable
y <- train$SalePrice
```

Set the seed for reproducibility

```{r}
set.seed(42)

```

### Split the data into training and test sets

```{r}
train_indices <- sample(1:nrow(train), size = round(0.8 * nrow(train)), replace = FALSE)
X_train <- X[train_indices, ]
y_train <- y[train_indices]

X_test <- X[-train_indices, ]
y_test <- y[-train_indices]
```

### Standardize Features

Fit the scaler on the training data

```{r}

scaler <- scale(X_train)

# Apply the scaler to the training data
X_train_scaled <- scaler

# Apply the scaler to the test data
X_test_scaled <- scale(X_test, center = attr(scaler, "scaled:center"), scale = attr(scaler, "scaled:scale"))
```

Calculate the mean and standard deviation from the training set:

```{r}
scale_params <- apply(X_train, 2, mean)
scale_sd <- apply(X_train, 2, sd)

# Scale the training set
X_train_scaled <- scale(X_train, center = scale_params, scale = scale_sd)

# Scale the test set using the scaling parameters from the training set
X_test_scaled <- scale(X_test, center = scale_params, scale = scale_sd)
```

# MODELS

Let's build initial functions for the models:

```{r}
# build finction for loss function
rmse_cv <- function(model) {
  rmse <- sqrt(-mean(cross_val_score(model, X, y, scoring = "neg_mean_squared_error", cv = 5)))
  return(round(rmse, 4))
}

evaluation <- function(y, predictions) {
  mae <- mean(abs(y - predictions))
  mse <- mean((y - predictions)^2)
  rmse <- sqrt(mse)
  r_squared <- 1 - (sum((y - predictions)^2) / sum((y - mean(y))^2))
  
  return(list(mae = round(mae, 4), mse = round(mse, 4), rmse = round(rmse, 4), r_squared = round(r_squared, 4)))
}

models <- data.frame(Model = character(),
                     MAE = numeric(),
                     MSE = numeric(),
                     RMSE = numeric(),
                     R2_Score = numeric(),
                     RMSE_CV = numeric(),
                     stringsAsFactors = FALSE)
```

## 1. Linear Regression

Well, firstly create linear regression model

```{r}
## Create a linear regression model
lin_reg <- lm(y_train ~ ., data = X_train)

## Make predictions on the test set
ln_predictions <- predict(lin_reg, newdata = X_test)

## Evaluate the model
mae <- mean(abs(ln_predictions - y_test))
mse <- mean((ln_predictions - y_test)^2)
rmse <- sqrt(mse)
r_squared <- summary(lin_reg)$r.squared

### Print the evaluation metrics
print(paste("MAE:", mae))
print(paste("MSE:", mse))
print(paste("RMSE:", rmse))
print(paste("R2 Score:", r_squared))

## Perform cross-validation and calculate RMSE
rmse_cross_val <- sqrt(mean((predict(lin_reg, newdata = X_train) - y_train)^2))

### Print the cross-validated RMSE
print(paste("RMSE Cross-Validation:", rmse_cross_val))

## Create a data frame to store the model results
new_row <- data.frame(
  Model = "LinearRegression",
  MAE = mae,
  MSE = mse,
  RMSE = rmse,
  `R2 Score` = r_squared,
  `RMSE (Cross-Validation)` = rmse_cross_val,
  stringsAsFactors = FALSE  # Add this line to prevent factors
)

## Match the column names
colnames(new_row) <- colnames(models)

## Append the new row to the existing data frame (models)
models <- rbind(models, new_row)
```

## 2. Decision Tree

```{r}

## Train the decision tree model
d_tree <- rpart(y_train ~ ., data = X_train)

## Make predictions on the test set
predictions <- predict(d_tree, newdata = X_test)


## Evaluate the model
mae <- mean(abs(predictions - y_test))
mse <- mean((predictions - y_test)^2)
rmse <- sqrt(mse)
r_squared <- cor(predictions, y_test)^2

## Print the MAE
cat("MAE:", mae, "\n")

## Print the MSE
cat("MSE:", mse, "\n")

## Print the RMSE
cat("RMSE:", rmse, "\n")

## Print the R2 Score
cat("R2 Score:", r_squared, "\n")

## Perform cross-validation
cv_results <- rpart::rpart.control(cp = 0.01)  # Set the complexity parameter
cv_model <- rpart(y_train ~ ., data = X_train, control = cv_results)
cv_predictions <- predict(cv_model, newdata = X_train)

rmse_cross_val <- sqrt(mean((cv_predictions - y_train)^2))

## Print the RMSE Cross-Validation
cat("RMSE Cross-Validation:", round(rmse_cross_val, 2), "\n")
```

```{r}
## Create a new row for the model's results
new_row <- data.frame(
  Model = "DecisionTree",
  MAE = mae,
  MSE = mse,
  RMSE = rmse,
  `R2 Score` = r_squared,
  `RMSE (Cross-Validation)` = rmse_cross_val
)

## Append the new row to the existing models dataframe
models <- rbind(models, new_row)

```

## 3. Random Forest

```{r}
## Remove the 'Id' column and set row names for train and test datasets
row.names(train) <- train$Id
train$Id <- NULL
row.names(test) <- test$Id
test$Id <- NULL

## Split the 'train' dataset into features (X_train) and target variable (y_train)
X_train <- train[, -1]  # Exclude the first column (target variable)
y_train <- train[, 1]   # First column (target variable)

## Fit Random Forest model

train_data <- train[, -1]  # Exclude the first column (target variable)
y <- train[, 1]  # First column (target variable)

## Combine the features and target variable into train_data
train_data <- cbind(y, train_data)

random_forest <- randomForest(y ~ ., data = train_data, ntree = 250)
predictions <- predict(random_forest, newdata = test_data)

## Calculate evaluation metrics
mae <- mean(abs(test_data$y - predictions))
mse <- mean((test_data$y - predictions)^2)
rmse <- sqrt(mse)
r_squared <- 1 - sum((test_data$y - predictions)^2) / sum((test_data$y - mean(test_data$y))^2)

cat("MAE:", mae, "\n")
cat("MSE:", mse, "\n")
cat("RMSE:", rmse, "\n")
cat("R2 Score:", r_squared, "\n")
cat("----------------------------------------\n")

## Cross-validation using the caret package
ctrl <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation
rf_model <- train(y ~ ., data = train_data, method = "rf", trControl = ctrl)
rmse_cv <- sqrt(mean(rf_model$resample$RMSE))

cat("RMSE Cross-Validation:", rmse_cv, "\n")

## Add results to models data frame
new_row <- data.frame(Model = "RandomForest", MAE = mae, MSE = mse, RMSE = rmse, `R2 Score` = r_squared, `RMSE (Cross-Validation)` = rmse_cv)
models <- rbind(models, new_row)
```

## 4. Gradient Boosting Model

Codes for building model:

```{r}
# Fit the Gradient Boosting model
g_boost <- gbm(y_train ~ ., data = X_train, n.trees = 100, interaction.depth = 3)

# Predict on the test set
predictions <- predict(g_boost, newdata = X_test, n.trees = 100)

# Calculate evaluation metrics
metrics <- evaluation(y_test, predictions)
mae <- metrics$mae

mse <- metrics$mse
rmse <- metrics$rmse
r_squared <- metrics$r_squared

cat("MAE:", mae, "\n")
cat("MSE:", mse, "\n")
cat("RMSE:", rmse, "\n")
cat("R2 Score:", r_squared, "\n")


# Perform cross-validation using cv.glmnet
cv_fit <- cv.glmnet(x = as.matrix(X), y = y, nfolds = 5, alpha = 0.5)

# Calculate the RMSE
rmse_cv <- sqrt(cv_fit$cvm)

# Print the RMSE for each fold
cat("RMSE (Cross-Validation):", rmse_cv, "\n")

# Get the average RMSE
mean_rmse_cv <- mean(rmse_cv)
cat("Mean RMSE (Cross-Validation):", mean_rmse_cv, "\n")

# Append the average RMSE to the models data frame
models$`RMSE (Cross-Validation)`[models$Model == "GradientBoosting"] <- mean_rmse_cv


# Create a new row for the model results
new_row <- data.frame(Model = "GradientBoosting",
                      MAE = mae,
                      MSE = mse,
                      RMSE = rmse,
                      R2_Score = r_squared,
                      RMSE_CV = mean_rmse_cv)

# Append the new row to the models data frame
models <- rbind(models, new_row)
```

## 5. Extreme Gradient Boosting (XGBoost)

```{r}
# Convert the data to DMatrix format
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
dtest <- xgb.DMatrix(data = as.matrix(X_test))

# Set the parameters for the XGBoost model
params <- list(
  objective = "reg:squarederror",
  eval_metric = "rmse"
)

# Train the XGBoost model
Xg_boost <- xgb.train(params = params, data = dtrain, nrounds = 100)

# Predict on the test set
predictions <- predict(Xg_boost, newdata = dtest)

# Calculate evaluation metrics
mae <- mean(abs(predictions - y_test))
mse <- mean((predictions - y_test)^2)
rmse <- sqrt(mse)
r_squared <- 1 - mse / var(y_test)

cat("MAE:", mae, "\n")
cat("MSE:", mse, "\n")
cat("RMSE:", rmse, "\n")
cat("R2 Score:", r_squared, "\n")


# Perform cross-validation to calculate RMSE
cv_result <- xgb.cv(params = params, data = dtrain, nfold = 5, nrounds = 100)
rmse_cross_val <- min(cv_result$evaluation_log$test_rmse_mean)

cat("RMSE Cross-Validation:", rmse_cross_val, "\n")

# Create a new row for the model results
new_row <- data.frame(Model = "XGradientBoosting",
                      MAE = mae,
                      MSE = mse,
                      RMSE = rmse,
                      R2_Score = r_squared,
                      RMSE_CV = rmse_cross_val)

# Append the new row to the models data frame
models <- rbind(models, new_row)

```

## 6. Adaptive Boosting

```{r}
## Split the dataset into predictor variables and target variable
X_train <- train[, -which(names(train) == "y_train")]
y_train <- train$y_train

## Fit the AdaBoost model
Ada_boost <- boosting(y_train ~ ., data = train, boos = TRUE, mfinal = 250)


# Make predictions on the test set
predictions <- predict.boosting(Ada_boost, newdata = test)

## Make predictions on the test set
predictions <- predict.boosting(Ada_boost, newdata = test)

## Define the evaluation function
evaluation <- function(actual, predicted) {
  mae <- mean(abs(actual - predicted))
  mse <- mean((actual - predicted)^2)
  rmse <- sqrt(mse)
  r_squared <- 1 - (sum((actual - predicted)^2) / sum((actual - mean(actual))^2))
  return(list(mae = mae, mse = mse, rmse = rmse, r_squared = r_squared))
}

## Evaluate the predictions
eval_metrics <- evaluation(y_test, predictions)
mae <- eval_metrics$mae
mse <- eval_metrics$mse
rmse <- eval_metrics$rmse
r_squared <- eval_metrics$r_squared

## Define the cross-validation function
rmse_cv <- function(model) {
  rmse <- sqrt(mean((y_train - predict(model, X_train))^2))
  return(rmse)
}

## Calculate the cross-validated RMSE
rmse_cross_val <- rmse_cv(lin_reg)

## Create a dataframe for models
models <- data.frame(
  Model = "AdaptiveBoosting",
  MAE = mae,
  MSE = mse,
  RMSE = rmse,
  R2_Score = r_squared,
  RMSE_Cross_Validation = rmse_cross_val
)

## Print the evaluation metrics
cat("MAE:", mae, "\n")
cat("MSE:", mse, "\n")
cat("RMSE:", rmse, "\n")
cat("R2 Score:", r_squared, "\n")
cat("-----------------------------\n")
cat("RMSE Cross-Validation:", rmse_cross_val, "\n")
```

## 7. Support Vector Regression (SVR)

```{r}
# Fit the SVR model
svr <- svm(X_train, y_train, kernel = "radial", cost = 100000)

# Predict on the test set
predictions <- predict(svr, X_test)

# Calculate evaluation metrics
mae <- mean(abs(predictions - y_test))
mse <- mean((predictions - y_test)^2)

rmse <- sqrt(mse)
r_squared <- 1 - mse / var(y_test)

cat("MAE:", mae, "\n")
cat("MSE:", mse, "\n")
cat("RMSE:", rmse, "\n")
cat("R2 Score:", r_squared, "\n")


# Perform cross-validation to calculate RMSE
set.seed(42)
folds <- 5
cv_results <- vector("double", folds)

# Split the data into folds and perform cross-validation
for (i in 1:folds) {
  fold_indices <- seq(from = 1, to = nrow(X), length.out = folds + 1)
  fold_indices <- fold_indices[i:(i+1)]
  train_indices <- setdiff(1:nrow(X), fold_indices)
  
  fold_X_train <- X[train_indices, ]
  fold_y_train <- y[train_indices]
  fold_X_test <- X[fold_indices, ]
  fold_y_test <- y[fold_indices]
  
  fold_svr <- svm(fold_X_train, fold_y_train, kernel = "radial", cost = 100000)
  fold_predictions <- predict(fold_svr, fold_X_test)
  cv_results[i] <- sqrt(mean((fold_predictions - fold_y_test)^2))
}

cv_rmse <- mean(cv_results)

cat("RMSE Cross-Validation:", cv_rmse, "\n")

# Create a new row for the model results
new_row <- data.frame(Model = "SVR",
                      MAE = mae,
                      MSE = mse,
                      RMSE = rmse,
                      R2_Score = r_squared,
                      RMSE_CV = cv_rmse)

# Append the new row to the models data frame
models <- rbind(models, new_row)
```

# Comparison of the results of the models

```{r}
comparison_models <- models[order(models$RMSE), ]
comparison_models
```